# app_ocr_pro.py
import io, base64, os, sys, statistics, math
from typing import Tuple, Dict, Any, List, Optional
from flask import Flask, request, jsonify, render_template_string
from PIL import Image, ImageOps, UnidentifiedImageError
import pytesseract
import cv2
import numpy as np

# (Tu·ª≥ ch·ªçn) Windows: ch·ªâ ƒë·ªãnh ƒë∆∞·ªùng d·∫´n tesseract n·∫øu PATH ch∆∞a c√≥
# if sys.platform.startswith("win") and os.path.exists(r"C:\Program Files\Tesseract-OCR\tesseract.exe"):
#     pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

app = Flask(__name__)

HTML = r"""
<!doctype html>
<html lang="vi">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>OCR demo ‚Äî Ch·ª•p/Upload ·∫£nh ‚Üí In to√†n b·ªô text</title>
<style>
  :root { color-scheme: light dark; }
  body { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin: 16px; line-height: 1.45; }
  .wrap { max-width: 980px; margin: 0 auto; display: grid; gap: 16px; }
  .row { display: grid; gap: 16px; grid-template-columns: 1fr 1fr; }
  @media (max-width: 900px){ .row { grid-template-columns: 1fr; } }
  .card { border: 1px solid #5553; border-radius: 12px; padding: 12px; }
  video, img, canvas { width: 100%; border: 1px solid #5553; border-radius: 10px; }
  textarea { width: 100%; min-height: 260px; border-radius: 10px; border: 1px solid #5555; padding: 10px; }
  button { padding: 10px 14px; border-radius: 10px; border: 1px solid #4445; background: #eee; cursor: pointer; }
  .row-btns { display: flex; gap: 8px; flex-wrap: wrap; }
  .muted { opacity: .75; font-size: 13px; }
</style>
</head>
<body>
<div class="wrap">
  <h2>üì∑ OCR demo ‚Äî Ch·ª•p/Upload ·∫£nh ‚Üí In to√†n b·ªô text</h2>

  <div class="row">
    <div class="card">
      <h3>1) Ch·ª•p ·∫£nh b·∫±ng camera</h3>
      <video id="video" autoplay playsinline muted></video>
      <div class="row-btns">
        <button id="btnStart">B·∫≠t camera</button>
        <button id="btnSnap">Ch·ª•p ·∫£nh</button>
        <button id="btnSend">G·ª≠i ·∫£nh ƒë·ªÉ OCR</button>
      </div>
      <canvas id="canvas" width="1280" height="720" style="display:none"></canvas>
      <img id="preview" alt="Xem tr∆∞·ªõc ·∫£nh ch·ª•p" />
      <div class="muted">Camera ch·ªâ ho·∫°t ƒë·ªông tr√™n <b>http://localhost</b> ho·∫∑c <b>HTTPS</b>.</div>
    </div>

    <div class="card">
      <h3>2) Upload ·∫£nh t·ª´ m√°y</h3>
      <form id="formUpload">
        <input type="file" name="image" id="file" accept="image/*" required />
        <div class="row-btns"><button type="submit">Upload & OCR</button></div>
      </form>
      <div class="muted">Kh√¥ng gi·ªõi h·∫°n lo·∫°i ·∫£nh; server s·∫Ω c·ªë g·∫Øng ƒë·ªçc.</div>
    </div>
  </div>

  <div class="card">
    <h3>K·∫øt qu·∫£ OCR (to√†n b·ªô text)</h3>
    <textarea id="out" placeholder="Text OCR s·∫Ω hi·ªÉn th·ªã ·ªü ƒë√¢y..."></textarea>
    <div class="muted" id="meta"></div>
  </div>

  <div class="muted">
    M·∫πo: n·∫øu ch·ªâ c·∫ßn 1 nh√≥m k√Ω t·ª± (vd. m√£/ s·ªë), th√™m query <code>?wl=0-9A-Z/-.</code> v√†o URL ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c.
  </div>
</div>

<script>
const $ = (q)=>document.querySelector(q);
const video = $('#video'), canvas = $('#canvas'), preview = $('#preview'), out = $('#out'), meta = $('#meta');

let stream = null;
$('#btnStart').addEventListener('click', async ()=>{
  try{
    stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' }, audio: false });
    video.srcObject = stream;
  }catch(e){ alert('Kh√¥ng truy c·∫≠p ƒë∆∞·ª£c camera: ' + e); }
});

$('#btnSnap').addEventListener('click', ()=>{
  if(!stream){ alert('B·∫≠t camera tr∆∞·ªõc.'); return; }
  const w = video.videoWidth || 1280, h = video.videoHeight || 720;
  canvas.width = w; canvas.height = h;
  canvas.getContext('2d').drawImage(video, 0, 0, w, h);
  preview.src = canvas.toDataURL('image/jpeg');
});

$('#btnSend').addEventListener('click', async ()=>{
  if(!preview.src){ alert('H√£y ch·ª•p ·∫£nh tr∆∞·ªõc.'); return; }
  const b64 = preview.src.split(',')[1];
  const r = await fetch('/ocr_base64' + location.search, {method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify({image_base64:b64})});
  const data = await r.json();
  out.value = data.error ? ('L·ªói: ' + data.error) : (data.text || '');
  meta.textContent = data.error ? '' : (`pipeline: ${data.pipeline}, psm: ${data.psm}, conf‚âà${data.confidence}`);
});

$('#formUpload').addEventListener('submit', async (e)=>{
  e.preventDefault();
  const fd = new FormData(e.target);
  const r = await fetch('/ocr' + location.search, { method:'POST', body: fd });
  const data = await r.json();
  out.value = data.error ? ('L·ªói: ' + data.error) : (data.text || '');
  meta.textContent = data.error ? '' : (`pipeline: ${data.pipeline}, psm: ${data.psm}, conf‚âà${data.confidence}`);
});
</script>
</body>
</html>
"""

# ----------------- UTIL & PREPROCESS -----------------

def _fix_orientation(pil: Image.Image) -> Image.Image:
    try:
        return ImageOps.exif_transpose(pil)
    except Exception:
        return pil

def _resize_for_ocr(img: np.ndarray, target_long_side: int = 2000) -> np.ndarray:
    h, w = img.shape[:2]
    long_side = max(h, w)
    if long_side >= target_long_side:
        return img
    scale = target_long_side / float(long_side)
    new_w, new_h = int(w * scale), int(h * scale)
    return cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)

def _deskew(gray: np.ndarray) -> np.ndarray:
    try:
        bw = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]
        coords = np.column_stack(np.where(bw > 0))
        if coords.size == 0: return gray
        angle = cv2.minAreaRect(coords)[-1]
        angle = -(90 + angle) if angle < -45 else -angle
        (h, w) = gray.shape[:2]
        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1.0)
        return cv2.warpAffine(gray, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)
    except Exception:
        return gray

def _clahe(gray: np.ndarray) -> np.ndarray:
    return cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8)).apply(gray)

def _denoise(gray: np.ndarray) -> np.ndarray:
    # Bilateral gi·ªØ bi√™n, n·∫øu ch·∫≠m c√≥ th·ªÉ ƒë·ªïi sang medianBlur(3)
    return cv2.bilateralFilter(gray, d=7, sigmaColor=60, sigmaSpace=60)

def _sharpen(gray: np.ndarray) -> np.ndarray:
    blur = cv2.GaussianBlur(gray, (0,0), 1.0)
    sharp = cv2.addWeighted(gray, 1.6, blur, -0.6, 0)
    return np.clip(sharp, 0, 255).astype(np.uint8)

def _morph_open_close(bin_img: np.ndarray) -> np.ndarray:
    # m·ªü r·ªìi ƒë√≥ng ƒë·ªÉ x√≥a nhi·ªÖu nh·ªè v√† si·∫øt n√©t ch·ªØ
    k1 = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))
    k2 = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))
    opened = cv2.morphologyEx(bin_img, cv2.MORPH_OPEN, k1, iterations=1)
    closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, k2, iterations=1)
    return closed

def _pipelines(pil: Image.Image) -> List[Tuple[str, Image.Image]]:
    """T·∫°o nhi·ªÅu bi·∫øn th·ªÉ ·∫£nh ƒë·ªÉ th·ª≠ OCR; tr·∫£ (t√™n, ·∫£nh PIL)."""
    pil = _fix_orientation(pil.convert("RGB"))
    rgb = np.array(pil)
    rgb = _resize_for_ocr(rgb, target_long_side=2100)

    gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)
    gray = _deskew(gray)
    gray = _clahe(gray)
    gray = _denoise(gray)
    gray = _sharpen(gray)

    # Otsu + morphology
    _, th_otsu = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    th_otsu = _morph_open_close(th_otsu)

    # Adaptive (Gaussian)
    th_adp = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                   cv2.THRESH_BINARY, 33, 11)
    th_adp = _morph_open_close(th_adp)

    return [
        ("gray_clahe_sharp", Image.fromarray(gray)),
        ("binary_otsu_morph", Image.fromarray(th_otsu)),
        ("binary_adapt_morph", Image.fromarray(th_adp)),
    ]

# ------------- TESSERACT CONFIGS & OCR CORE -------------

BASE_CFG = "-c preserve_interword_spaces=1 -c user_defined_dpi=300"
TESS_CFGS = [
    ("6",  f"{BASE_CFG} --oem 3 --psm 6"),   # kh·ªëi vƒÉn b·∫£n
    ("4",  f"{BASE_CFG} --oem 3 --psm 4"),   # nhi·ªÅu c·ªôt
    ("11", f"{BASE_CFG} --oem 3 --psm 11"),  # th∆∞a/r·∫£i r√°c
    ("3",  f"{BASE_CFG} --oem 3 --psm 3"),   # auto fully
]

def _ocr_with_conf(pil_img: Image.Image, lang: str, config: str) -> Tuple[str, float]:
    text = pytesseract.image_to_string(pil_img, lang=lang, config=config)
    # l·∫•y conf trung b√¨nh t·ª´ image_to_data
    data = pytesseract.image_to_data(pil_img, lang=lang, config=config, output_type=pytesseract.Output.DICT)
    confs = []
    for c in data.get("conf", []):
        try:
            c = float(c)
            if c >= 0: confs.append(c)
        except Exception:
            pass
    avg = round(statistics.mean(confs), 1) if confs else 0.0
    return text, avg

def _best_ocr(pil: Image.Image, whitelist: Optional[str] = None) -> Dict[str, Any]:
    langs_to_try = ["vie+eng", "eng"]
    best = {"text":"", "confidence":0.0, "pipeline":"", "psm":""}

    # Th√™m whitelist n·∫øu c√≥ (l∆∞u √Ω: ch·ªâ d√πng khi b·∫°n ch·∫Øc ki·ªÉu k√Ω t·ª±)
    wl_cfg = ""
    if whitelist:
        wl_cfg = f" -c tessedit_char_whitelist={whitelist} "

    for lang in langs_to_try:
        for pname, pimg in _pipelines(pil):
            for psm, cfg in TESS_CFGS:
                try:
                    text, conf = _ocr_with_conf(pimg, lang, cfg + wl_cfg)
                except pytesseract.TesseractError:
                    continue
                # score = conf (c√≥ th·ªÉ c·ªông th√™m len(text) ƒë·ªÉ tr√°nh conf cao nh∆∞ng text ng·∫Øn b·∫•t th∆∞·ªùng)
                score = conf + min(len(text)/300.0, 5)  # ∆∞u ti√™n text d√†i h∆°n m·ªôt ch√∫t
                if score > best["confidence"]:
                    best = {"text": text, "confidence": round(conf,1), "pipeline": pname, "psm": psm}
        if best["text"].strip():
            break
    return best

# -------------------------- ROUTES --------------------------

@app.route("/")
def index():
    return render_template_string(HTML)

@app.route("/ocr", methods=["POST"])
def ocr_upload():
    wl = request.args.get("wl")  # t√πy ch·ªçn whitelist, v√≠ d·ª• wl=0-9A-Z/-. 
    if "image" not in request.files:
        return jsonify({"error":"Thi·∫øu file ·∫£nh (field name: image)."}), 400
    f = request.files["image"]
    if not f or f.filename == "":
        return jsonify({"error":"File ·∫£nh tr·ªëng."}), 400
    try:
        pil = Image.open(io.BytesIO(f.read()))
    except UnidentifiedImageError:
        return jsonify({"error":"File kh√¥ng ph·∫£i ·∫£nh h·ª£p l·ªá."}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500

    result = _best_ocr(pil, whitelist=wl)
    return jsonify({"text": result["text"], "confidence": result["confidence"], "pipeline": result["pipeline"], "psm": result["psm"]})

@app.route("/ocr_base64", methods=["POST"])
def ocr_base64():
    wl = request.args.get("wl")
    data = request.get_json(silent=True) or {}
    b64 = data.get("image_base64")
    if not b64:
        return jsonify({"error":"Thi·∫øu image_base64."}), 400
    try:
        pil = Image.open(io.BytesIO(base64.b64decode(b64)))
    except UnidentifiedImageError:
        return jsonify({"error":"Chu·ªói base64 kh√¥ng ph·∫£i ·∫£nh h·ª£p l·ªá."}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500

    result = _best_ocr(pil, whitelist=wl)
    return jsonify({"text": result["text"], "confidence": result["confidence"], "pipeline": result["pipeline"], "psm": result["psm"]})

if __name__ == "__main__":
    # Dev: localhost ƒë·ªÉ test camera; khi public: Waitress/Caddy (HTTPS).
    app.run(host="0.0.0.0", port=8080, debug=False)
